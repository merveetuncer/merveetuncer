{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "language_model.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNOU7W5Z41QPfcvq0l3IEmU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/merveetuncer/merveetuncer/blob/master/language_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jwtCbguVCKJl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Transform the list of sentences into a list of words\n",
        "all_words = ' '.join(sheldon_quotes).split(' ')\n",
        "\n",
        "# Get number of unique words\n",
        "unique_words = list(set(all_words))\n",
        "\n",
        "# Dictionary of indexes as keys and words as values\n",
        "index_to_word = {i:wd for i, wd in enumerate(sorted(unique_words))}\n",
        "\n",
        "print(index_to_word)\n",
        "\n",
        "# Dictionary of words as keys and indexes as values\n",
        "word_to_index = {wd:i for i,wd in enumerate(sorted(unique_words))}\n",
        "\n",
        "print(word_to_index)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JWHzjFpcCN2F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create lists to keep the sentences and the next character\n",
        "sentences = []   # ~ Training data\n",
        "next_chars = []  # ~ Training labels\n",
        "\n",
        "# Define hyperparameters\n",
        "step = 2          # ~ Step to take when reading the texts in characters\n",
        "chars_window = 10 # ~ Number of characters to use to predict the next one  \n",
        "\n",
        "# Loop over the text: length `chars_window` per time with step equal to `step`\n",
        "for i in range(0, len(sheldon_quotes) - chars_window, step):\n",
        "    sentences.append(sheldon_quotes[i:i + chars_window])\n",
        "    next_chars.append(sheldon_quotes[i+chars_window])\n",
        "\n",
        "# Print 10 pairs\n",
        "print_examples(sentences, next_chars, 10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s8S4FsIlCupJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Loop through the sentences and get indexes\n",
        "new_text_split = []\n",
        "for sentence in new_text:\n",
        "    sent_split = []\n",
        "    for wd in sentence.split(' '):\n",
        "        index = word_to_index.get(wd, 0)\n",
        "        sent_split.append(index)\n",
        "    new_text_split.append(sent_split)\n",
        "\n",
        "# Print the first sentence's indexes\n",
        "print(new_text_split[0])\n",
        "\n",
        "# Print the sentence converted using the dictionary\n",
        "print(' '.join([index_to_word[index] for index in new_text_split[0]]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U1wa8lxOMM_P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import relevant classes/functions\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Build the dictionary of indexes\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(texts)\n",
        "\n",
        "# Change texts into sequence of indexes\n",
        "texts_numeric = tokenizer.texts_to_sequences(texts)\n",
        "print(\"Number of words in the sample texts: ({0}, {1})\".format(len(texts_numeric[0]), len(texts_numeric[1])))\n",
        "\n",
        "# Pad the sequences\n",
        "texts_pad = pad_sequences(texts_numeric, 60)\n",
        "print(\"Now the texts have fixed length: 60. Let's see the first one: \\n{0}\".format(texts_pad[0]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hRKHeQNSMNl-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Build model\n",
        "model = Sequential()\n",
        "model.add(SimpleRNN(units=128, input_shape=(None, 1)))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.compile(loss='binary_crossentropy', \n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Load pre-trained weights\n",
        "model.load_weights('model_weights.h5')\n",
        "\n",
        "# Method '.evaluate()' shows the loss and accuracy\n",
        "loss, acc = model.evaluate(x_test, y_test, verbose=0)\n",
        "print(\"Loss: {0} \\nAccuracy: {1}\".format(loss, acc))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v77Vqux8MvRL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create a Keras model with one hidden Dense layer\n",
        "model = Sequential()\n",
        "model.add(Dense(25, input_dim=20, activation='relu', kernel_initializer=he_uniform(seed=42)))\n",
        "model.add(Dense(1, activation='linear'))\n",
        "\n",
        "# Compile and fit the model\n",
        "model.compile(loss='mean_squared_error', optimizer=SGD(lr=0.01, momentum=0.9, clipvalue=3.0))\n",
        "history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=100, verbose=0)\n",
        "\n",
        "# See Mean Square Error for train and test data\n",
        "train_mse = model.evaluate(X_train, y_train, verbose=0)\n",
        "test_mse = model.evaluate(X_test, y_test, verbose=0)\n",
        "\n",
        "# Print the values of MSE\n",
        "print('Train: %.3f, Test: %.3f' % (train_mse, test_mse))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XDrD2_mtrR0g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create the model\n",
        "model = Sequential()\n",
        "model.add(SimpleRNN(units=600, input_shape=(None, 1)))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.compile(loss='binary_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
        "\n",
        "# Load pre-trained weights\n",
        "model.load_weights('model_weights.h5')\n",
        "\n",
        "# Plot the accuracy x epoch graph\n",
        "plt.plot(history.history['acc'])\n",
        "plt.plot(history.history['val_acc'])\n",
        "plt.legend(['train', 'val'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "prwZC-Jlrnw4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import the modules\n",
        "from keras.layers import GRU, Dense\n",
        "\n",
        "# Print the old and new model summaries\n",
        "SimpleRNN_model.summary()\n",
        "gru_model.summary()\n",
        "\n",
        "# Evaluate the models' performance (ignore the loss value)\n",
        "_, acc_simpleRNN = SimpleRNN_model.evaluate(X_test, y_test, verbose=0)\n",
        "_, acc_GRU = gru_model.evaluate(X_test, y_test, verbose=0)\n",
        "\n",
        "# Print the results\n",
        "print(\"SimpleRNN model's accuracy:\\t{0}\".format(acc_simpleRNN))\n",
        "print(\"GRU model's accuracy:\\t{0}\".format(acc_GRU))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "id_keO9s89NA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import the LSTM layer\n",
        "from keras.layers.recurrent import LSTM\n",
        "# Build model\n",
        "model = Sequential()\n",
        "model.add(LSTM(units=128, input_shape=(None, 1), return_sequences=True))\n",
        "model.add(LSTM(units=128, return_sequences=True))\n",
        "model.add(LSTM(units=128, return_sequences=False))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Load pre-trained weights\n",
        "model.load_weights('lstm_stack_model_weights.h5')\n",
        "\n",
        "print(\"Loss: %0.04f\\nAccuracy: %0.04f\" % tuple(model.evaluate(X_test, y_test, verbose=0)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NZi-d2xR9f4c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import the embedding layer\n",
        "from keras.layers import Embedding\n",
        "\n",
        "# Create a model with embeddings\n",
        "model = Sequential(name=\"emb_model\")\n",
        "model.add(Embedding(input_dim=80002, output_dim=wordvec_dim, input_length=200, trainable=True))\n",
        "model.add(GRU(128))\n",
        "model.add(Dense(1))\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Print the summaries of the one-hot model\n",
        "model_onehot.summary()\n",
        "\n",
        "# Print the summaries of the model with embeddings\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YXn1eUpP_bCn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load the glove pre-trained vectors\n",
        "glove_matrix = load_glove('glove_200d.zip')\n",
        "\n",
        "# Create a model with embeddings\n",
        "model = Sequential(name=\"emb_model\")\n",
        "model.add(Embedding(input_dim=vocabulary_size + 1, output_dim=wordvec_dim, \n",
        "                    embeddings_initializer=Constant(glove_matrix), \n",
        "                    input_length=sentence_len, trainable=False))\n",
        "model.add(GRU(128))\n",
        "model.add(Dense(1))\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Print the summaries of the model with embeddings\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ylkgMd2H_vNm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create the model with embedding\n",
        "model = Sequential(name=\"emb_model\")\n",
        "model.add(Embedding(input_dim=max_vocabulary, output_dim=wordvec_dim, input_length=max_len))\n",
        "model.add(SimpleRNN(units=128))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Load pre-trained weights\n",
        "model.load_weights('embedding_model_weights.h5')\n",
        "\n",
        "# Evaluate the models' performance (ignore the loss value)\n",
        "_, acc_embeddings = model.evaluate(X_test, y_test, verbose=0)\n",
        "\n",
        "# Print the results\n",
        "print(\"SimpleRNN model's accuracy:\\t{0}\\nEmbeddings model's accuracy:\\t{1}\".format(acc_simpleRNN, acc_embeddings))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kbYWZCzP_9zN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Build and compile the model\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocabulary_size, wordvec_dim, trainable=True, input_length=max_text_len))\n",
        "model.add(LSTM(64, return_sequences=True, dropout=0.2, recurrent_dropout=0.15))\n",
        "model.add(LSTM(64, return_sequences=False, dropout=0.2, recurrent_dropout=0.15))\n",
        "model.add(Dense(16))\n",
        "model.add(Dropout(rate=0.25))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Load pre-trained weights\n",
        "model.load_weights('model_weights.h5')\n",
        "\n",
        "# Print the obtained loss and accuracy\n",
        "print(\"Loss: {0}\\nAccuracy: {1}\".format(*model.evaluate(X_test, y_test, verbose=0)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nxViAV1IBV2N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Print the model summary\n",
        "model_cnn.summary()\n",
        "\n",
        "# Load pre-trained weights\n",
        "model_cnn.load_weights('model_weights.h5')\n",
        "\n",
        "# Evaluate the model to get the loss and accuracy values\n",
        "loss, acc = model_cnn.evaluate(x_test, y_test, verbose=0)\n",
        "\n",
        "# Print the loss and accuracy obtained\n",
        "print(\"Loss: {0}\\nAccuracy: {1}\".format(loss, acc))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KvhPGOvQBiBS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get the numerical ids of column label\n",
        "numerical_ids = df.label.cat.codes\n",
        "\n",
        "# Print initial shape\n",
        "print(numerical_ids.shape)\n",
        "\n",
        "# One-hot encode the indexes\n",
        "Y = to_categorical(numerical_ids)\n",
        "\n",
        "# Check the new shape of the variable\n",
        "print(Y.shape)\n",
        "\n",
        "# Print the first 5 rows\n",
        "print(Y[0:5])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BblvXA87glg9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create and fit tokenizer\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(news_dataset)\n",
        "\n",
        "# Prepare the data\n",
        "prep_data = tokenizer.texts_to_sequences(news_dataset.data)\n",
        "prep_data = pad_sequences(prep_data, maxlen=200)\n",
        "\n",
        "# Prepare the labels\n",
        "prep_labels = to_categorical(news_dataset.target)\n",
        "\n",
        "# Print the shapes\n",
        "print(prep_data.shape)\n",
        "print(prep_labels.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hjX_hOGtg_sY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import plotting package\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Insert lists of accuracy obtained on the validation set\n",
        "plt.plot(history_no_emb['acc'], marker='o')\n",
        "plt.plot(history_emb['acc'], marker='o')\n",
        "\n",
        "# Add extra descriptions to plot\n",
        "plt.title('Learning with and without pre-trained embedding vectors')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['no_embeddings', 'with_embeddings'], loc='upper left')\n",
        "\n",
        "# Display the plot\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ATJQio2ei84K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Word2Vec model\n",
        "w2v_model = Word2Vec.load('bigbang_word2vec.model')\n",
        "\n",
        "# Selected words to check similarities\n",
        "words_of_interest =[\"bazinga\", \"penny\", \"universe\", \"spock\", \"brain\"]\n",
        "\n",
        "# Compute top 5 similar words for each of the words of interest\n",
        "top5_similar_words = []\n",
        "for word in words_of_interest:\n",
        "    top5_similar_words.append(\n",
        "      {word: [item[0] for item in w2v_model.wv.most_similar([word], topn=5)]}\n",
        "    )\n",
        "\n",
        "# Print the similar words\n",
        "print (top5_similar_words)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9TTCWCvUjefH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " See example article\n",
        "print(news_dataset.data[5])\n",
        "\n",
        "# Transform the text into numerical indexes\n",
        "news_num_indices = tokenizer.texts_to_sequences(news_dataset.data)\n",
        "\n",
        "# Print the transformed example article\n",
        "print(news_num_indices[5])\n",
        "\n",
        "# Transform the labels into one-hot encoded vectors\n",
        "labels_onehot = to_categorical(news_dataset.target)\n",
        "\n",
        "# Check before and after for the sample article\n",
        "print(\"Before: {0}\\nAfter: {1}\".format(news_dataset.target[5], labels_onehot[5]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "smGh6YNAlsKV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Change text for numerical ids and pad\n",
        "X_novel = tokenizer.texts_to_sequences(news_novel.data)\n",
        "X_novel = pad_sequences(X_novel, maxlen=400)\n",
        "\n",
        "# One-hot encode the labels\n",
        "Y_novel = to_categorical(news_novel.target)\n",
        "\n",
        "# Load the model pre-trained weights\n",
        "model.load_weights('classify_news_weights.h5')\n",
        "\n",
        "# Evaluate the model on the new dataset\n",
        "loss, acc = model.evaluate(X_novel, Y_novel, batch_size=64)\n",
        "\n",
        "# Print the loss and accuracy obtained\n",
        "print(\"Loss:\\t{0}\\nAccuracy:\\t{1}\".format(loss, acc))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i2UasTZkmpc0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get probabilities for each class\n",
        "pred_probabilities = model.predict_proba(X_test)\n",
        "\n",
        "# Thresholds at 0.5 and 0.8\n",
        "y_pred_50 = [np.argmax(x) if np.max(x) >= 0.5 else DEFAULT_CLASS for x in pred_probabilities]\n",
        "y_pred_80 = [np.argmax(x) if np.max(x) >= 0.8 else DEFAULT_CLASS for x in pred_probabilities]\n",
        "\n",
        "trade_off = pd.DataFrame({\n",
        "    'Precision_50': precision_score(y_true, y_pred_50, average=None), \n",
        "    'Precision_80': precision_score(y_true, y_pred_80, average=None), \n",
        "    'Recall_50': recall_score(y_true, y_pred_50, average=None), \n",
        "    'Recall_80': recall_score(y_true, y_pred_80, average=None)}, \n",
        "  index=['Class 1', 'Class 2', 'Class 3'])\n",
        "\n",
        "print(trade_off)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P9isPKCen8Zu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Use the model to predict on new data\n",
        "predicted = model.predict(X_test)\n",
        "\n",
        "# Choose the class with higher probability \n",
        "y_pred = np.argmax(predicted, axis=1)\n",
        "\n",
        "# Compute and print the confusion matrix\n",
        "print(confusion_matrix(y_true, y_pred))\n",
        "\n",
        "# Create the performance report\n",
        "print(classification_report(y_true, y_pred, target_names=news_cat))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YexbBwRCo3zp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#sequence to sequence model\n",
        "# Context for Sheldon phrase\n",
        "sheldon_context = \"I’m not insane, my mother had me tested. \"\n",
        "\n",
        "# Generate one Sheldon phrase\n",
        "sheldon_phrase = generate_sheldon_phrase(sheldon_model, sheldon_context)\n",
        "\n",
        "# Print the phrase\n",
        "print(sheldon_phrase)\n",
        "\n",
        "# Context for poem\n",
        "poem_context = \"May thy beauty forever remain\"\n",
        "\n",
        "# Print the poem\n",
        "print(generate_poem(poem_model, poem_context))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NFD20shLtLPO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Transform text into sequence of indexes and pad\n",
        "X = encode_sequences(sentences)\n",
        "\n",
        "# Print the sequences of indexes\n",
        "print(X)\n",
        "\n",
        "# Translate the sentences\n",
        "translated = translate_many(model, X)\n",
        "\n",
        "# Create pandas DataFrame with original and translated\n",
        "df = pd.DataFrame({'Original': sentences, 'Translated': translated})\n",
        "\n",
        "# Print the DataFrame\n",
        "print(df)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eL_GBSOGtyfM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_next_char(model, initial_text, chars_window, char_to_index, index_to_char):\n",
        "  \t# Initialize the X vector with zeros\n",
        "    X = initialize_X(initial_text, chars_window, char_to_index)\n",
        "    \n",
        "    # Get next character using the model\n",
        "    next_char = predict_next_char(model, X, index_to_char)\n",
        "\t\n",
        "    return next_char\n",
        "\n",
        "# Define context sentence and print the generated text\n",
        "initial_text = \"I am not insane, \"\n",
        "print(\"Next character: {0}\".format(get_next_char(model, initial_text, 20, char_to_index, index_to_char)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M3aM2fh7wOSs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_phrase(model, initial_text):\n",
        "    # Initialize variables  \n",
        "    res, seq, counter, next_char = initialize_params(initial_text)\n",
        "    \n",
        "    # Loop until stop conditions are met\n",
        "    while counter < 100 and next_char != r'.':\n",
        "      \t# Get next char using the model and append to the sentence\n",
        "        next_char, res, seq = get_next_token(model, res, seq)\n",
        "        # Update the counter\n",
        "        counter = counter + 1\n",
        "    return res\n",
        "  \n",
        "# Create a phrase\n",
        "print(generate_phrase(model, \"I am not insane, \"))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TiM364Thwoq9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define the initial text\n",
        "initial_text = \"Spock and me \"\n",
        "\n",
        "# Define a vector with temperature values\n",
        "temperatures = [0.2, 0.8, 1.0, 3.0, 10.0]\n",
        "\n",
        "# Loop over temperatures and generate phrases\n",
        "for temperature in temperatures:\n",
        "\t# Generate a phrase\n",
        "\tphrase = generate_phrase(model, initial_text, temperature)\n",
        "    \n",
        "\t# Print the phrase\n",
        "\tprint('Temperature {0}: {1}'.format(temperature, phrase))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eESvH3pxw5K-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Instantiate the vectors\n",
        "sentences = []\n",
        "next_chars = []\n",
        "# Loop for every sentence\n",
        "for sentence in sheldon.split('\\n'):\n",
        "    # Get 20 previous chars and next char; then shift by step\n",
        "    for i in range(0, len(sentence) - chars_window, step):\n",
        "        sentences.append(sentence[i:i + chars_window])\n",
        "        next_chars.append(sentence[i + chars_window])\n",
        "\n",
        "# Define a Data Frame with the vectors\n",
        "df = pd.DataFrame({'sentence': sentences, 'next_char': next_chars})\n",
        "\n",
        "# Print the initial rows\n",
        "print(df.head())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xNcNwz0HyoZM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Instantiate the variables with zeros\n",
        "numerical_sentences = np.zeros((num_seqs, chars_window, n_vocab), dtype=np.bool)\n",
        "numerical_next_chars = np.zeros((num_seqs, n_vocab), dtype=np.bool)\n",
        "\n",
        "# Loop for every sentence\n",
        "for i, sentence in enumerate(sentences):\n",
        "  # Loop for every character in sentence\n",
        "  for t, char in enumerate(sentence):\n",
        "    # Set position of the character to 1\n",
        "    numerical_sentences[i, t, char_to_index[char]] = 1\n",
        "    # Set next character to 1\n",
        "    numerical_next_chars[i, char_to_index[next_chars[i]]] = 1\n",
        "\n",
        "# Print the first position of each\n",
        "print(numerical_sentences[0], numerical_next_chars[0], sep=\"\\n\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8DRV5rjq0VGy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Instantiate the model\n",
        "model = Sequential()\n",
        "\n",
        "# Add two LSTM layers\n",
        "model.add(LSTM(64, input_shape=input_shape, dropout=0.15, recurrent_dropout=0.15, return_sequences=True))\n",
        "model.add(LSTM(64, dropout=0.15, recurrent_dropout=0.15, return_sequences=False))\n",
        "\n",
        "# Add the output layer\n",
        "model.add(Dense(n_vocab, activation='softmax'))\n",
        "\n",
        "# Compile and load weights\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "model.load_weights('model_weights.h5')\n",
        "# Summary\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z74emcg40lAH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get maximum length of the sentences\n",
        "pt_length = max([len(sentence.split()) for sentence in pt_sentences])\n",
        "\n",
        "# Transform text to sequence of numerical indexes\n",
        "X = input_tokenizer.texts_to_sequences(pt_sentences)\n",
        "\n",
        "# Pad the sequences\n",
        "X = pad_sequences(X, maxlen=pt_length, padding='post')\n",
        "\n",
        "# Print first sentence\n",
        "print(pt_sentences[0])\n",
        "\n",
        "# Print transformed sentence\n",
        "print(X[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tv6UxEa42Jy2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Initialize the variable\n",
        "Y = transform_text_to_sequences(en_sentences, output_tokenizer)\n",
        "\n",
        "# Temporary list\n",
        "ylist = list()\n",
        "for sequence in Y:\n",
        "  \t# One-hot encode sentence and append to list\n",
        "    ylist.append(to_categorical(sequence, num_classes=en_vocab_size))\n",
        "\n",
        "# Update the variable\n",
        "Y = np.array(ylist).reshape(Y.shape[0], Y.shape[1], en_vocab_size)\n",
        "\n",
        "# Print the raw sentence and its transformed version\n",
        "print(\"Raw sentence: {0}\\nTransformed: {1}\".format(en_sentences[0], Y[0]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yv0SR5Jw3VTz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Function to predict many phrases\n",
        "def predict_many(model, sentences, index_to_word, raw_dataset):\n",
        "    for i, sentence in enumerate(sentences):\n",
        "        # Translate the Portuguese sentence\n",
        "        translation = predict_one(model, sentence, index_to_word)\n",
        "        \n",
        "        # Get the raw Portuguese and English sentences\n",
        "        raw_target, raw_src = raw_dataset[i]\n",
        "        \n",
        "        # Print the correct Portuguese and English sentences and the predicted\n",
        "        print('src=[%s], target=[%s], predicted=[%s]' % (raw_src, raw_target, translation))\n",
        "\n",
        "predict_many(model, X_test[0:10], en_index_to_word, test)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}